{
 "metadata": {
  "name": "",
  "signature": "sha256:98fc16a5fdfe81e8831d9ce076ab2dc4fecc50f7ff52c637fcf80580003ba53a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 1: Generate training set\n",
      "============================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.io import loadmat\n",
      "from numpy.random import randint\n",
      "from numpy.random import uniform\n",
      "\n",
      "def sampleImages():\n",
      "  mat = loadmat('IMAGES.mat')\n",
      "  images = mat['IMAGES']\n",
      "  patchSize = 8\n",
      "  numPatches = 10000\n",
      "  patches = np.zeros((patchSize*patchSize,numPatches))\n",
      "\n",
      "  for i in range(numPatches):\n",
      "    img = randint(0,9)\n",
      "    i,j = randint(0,503),randint(0,503)\n",
      "    patches[:,i] = images[i:i+patchSize,j:j+patchSize,img].reshape(patchSize*patchSize)\n",
      "\n",
      "  return normalizeData(patches)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalizeData(patches):\n",
      "  mean = patches.mean(axis=0)\n",
      "  patches = patches - mean\n",
      "  pstd = 3 * patches.std()\n",
      "  patches = np.maximum(np.minimum(patches, pstd), -pstd) / pstd\n",
      "  patches = (1 + patches) * 0.4 + 0.1\n",
      "  return patches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 2: Sparse autoencoder objective\n",
      "===================================="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a training set of $m$ examples, we then define the overall cost function to be:\n",
      "\n",
      "$$ \n",
      "\\begin{align}\n",
      "J(W,b)\n",
      "&= \\left[ \\frac{1}{m} \\sum_{i=1}^m J(W,b;x^{(i)},y^{(i)}) \\right]\n",
      "                       + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( W^{(l)}_{ji} \\right)^2\n",
      " \\\\\n",
      "&= \\left[ \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{1}{2} \\left\\| h_{W,b}(x^{(i)}) - y^{(i)} \\right\\|^2 \\right) \\right]\n",
      "                       + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( W^{(l)}_{ji} \\right)^2\n",
      "                       \\end{align}\n",
      "$$\n",
      "\n",
      "And the $J_{sparse}$ as:\n",
      "$$\n",
      "\\begin{align}\n",
      "J_{\\rm sparse}(W,b) = J(W,b) + \\beta \\sum_{j=1}^{s_2} {\\rm KL}(\\rho || \\hat\\rho_j),\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Backpropagation propagation algorithm can be written in the following way:\n",
      "\n",
      "\n",
      "1. Perform a feedforward pass, computing the activations for layers $\\textstyle L_2$, $\\textstyle L_3$, up to the output layer $\\textstyle L_{n_l}$, using the equations defining the forward propagation steps\n",
      "2. For the output layer (layer $\\textstyle n_l$), set \n",
      "$$\\begin{align}\n",
      "\\delta^{(n_l)}\n",
      "= - (y - a^{(n_l)}) \\bullet f'(z^{(n_l)})\n",
      "\\end{align}$$\n",
      "4. For $\\textstyle l = n_l-1, n_l-2, n_l-3, \\ldots, 2$ Set\n",
      "$$\n",
      "\\begin{align}\n",
      "\\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\bullet f'(z^{(l)})\n",
      "\\end{align}\n",
      "$$\n",
      "1. Compute the desired partial derivatives: \n",
      "$$\n",
      "\\begin{align}\n",
      "\\nabla_{W^{(l)}} J(W,b;x,y) &= \\delta^{(l+1)} (a^{(l)})^T, \\\\\n",
      "\\nabla_{b^{(l)}} J(W,b;x,y) &= \\delta^{(l+1)}.\n",
      "\\end{align}\n",
      "$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sparseAutoencoderCost(theta, visibleSize, hiddenSize, lamb,\n",
      "                          sparsityParam, beta, data):\n",
      "   #Unroll parameters \n",
      "   W1 = theta[0:hidden_size * visible_size].reshape(hidden_size, visible_size)\n",
      "   W2 = theta[hidden_size * visible_size:2 * hidden_size * visible_size].reshape(visible_size, hidden_size)\n",
      "   b1 = theta[2 * hidden_size * visible_size:2 * hidden_size * visible_size + hidden_size]\n",
      "   b2 = theta[2 * hidden_size * visible_size + hidden_size:]\n",
      "   \n",
      "    #Initialize\n",
      "   cost = 0\n",
      "   W1grad = np.zeros(shape(W1))\n",
      "   W2grad = np.zeros(shape(W2))\n",
      "   b1grad = np.zeros(shape(b1))\n",
      "   b2grad = np.zeros(shape(b2))\n",
      "   \n",
      "   m = len(data)\n",
      "   \n",
      "   #Forward propagation\n",
      "   z2 = W1.dot(data) + b\n",
      "   a2 = sigmoid(z2)\n",
      "   z3 = W2.dot(a2) + b\n",
      "   h = sigmoid(z3) #a3\n",
      "   \n",
      "   sparsityParamHat = (1/m)*sum(a2)\n",
      "   #cost function J(W,b,beta,lambda) \n",
      "   cost = (1/m)*sum((h-data)**2) + (lamb/2)*(sum(W2**2)+sum(W1**2)) + beta*sum(KL(sparsityParamHat,sparsityParam))\n",
      "    \n",
      "   #Backpropagation\n",
      "   delta3 = -(data-h)*sigmoid_prime()\n",
      "   delta2 = np.dot(W2.T,delta3)\n",
      "    \n",
      "   #compute the desired partial derivates\n",
      "   W1grad = delta2*a1.T\n",
      "   W2grad = delta3*a2.T\n",
      "   b1grad = delta2\n",
      "   b2grad = delta3 \n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where the activation function $f(z) = \\frac{1}{{1+e^{-z}}}$, i.e, the sigmoid function, $f'(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i)$ can be calculated in terms of previosly calculated values and we can extend the definition of $f(z)$ and $f'(z)$ in the following way: $f'([z_1, z_2, z_3]) =[f'(z_1),f'(z_2),f'(z_3)]$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(x):\n",
      "  return 1 / (1 + np.exp(-x))}\n",
      "    \n",
      "def sigmoiod_prime(x):\n",
      "  return a(1-a)\n",
      "\n",
      "def KL(x,y):\n",
      "  return x*np.log(x/y) + (1-x)*np.log((1- x)/(1-y))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 3: Gradient checking\n",
      "========================="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tenemos una funci\u00f3n $g$ que supueeestamente calcula $\\textstyle \\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; nos gustar\u00eda pillar si los valores de $g_i$ son correctos. Sea $ \\textstyle \\theta^{(i+)} = \\theta + {\\rm EPSILON} \\times \\vec{e}_i$, donde $\n",
      "\\vec{e}_i = \\begin{bmatrix}0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0\\end{bmatrix}\n",
      "$ es el i\u00e9simo vector base (un bector del mismo tama\u00f1o que $\\theta$, con un $1$ en la i\u00e9sima posici\u00f3n). Similarmente, sea $ \\textstyle \\theta^{(i-)} = \\theta - {\\rm EPSILON} \\times \\vec{e}_i$.\n",
      "\n",
      "Podemos al fin verificar la correctitud, para cada unidad $i$ deber\u00eda cumplirse que:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "g_i(\\theta) \\approx\n",
      "\\frac{J(\\theta^{(i+)}) - J(\\theta^{(i-)})}{2 \\times {\\rm EPSILON}}.\n",
      "\\end{align}\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def computeNumericalGradient(J,theta):\n",
      "    # theta: a vector of parameters\n",
      "    # J: a function that outputs a real-number. Calling y = J(theta) will return the\n",
      "    # function value at theta.\n",
      "    for i in xrange(size(theta)):\n",
      "        numgrad = np.zeros(size(theta))\n",
      "        epsilon = np.zeros(size(theta))\n",
      "        epsilon[i] = epsilon[i]+0.0001\n",
      "        numgrad[i] = (J(theta+epsilon)-J(theta-epsilon))/2*0.0001\n",
      "    return numgrad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 4: Train the sparse autoencoder\n",
      "===================================="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm (L-BFGS algorithm) implemantation is included in <code> scipy.optimize.fmin_l_bfgs_b </code>, we're interested only in three parameters of that implementation: <code> func </code> the function to minimize <code> callable f(x, *args) </code>, <code> x0 </code> our initial guess and <code> fprime </code> the gradient of <code> func </code>. For the purpose of this assignment (we don't need your education noooo, teacher live these kids alone) we don't need to know how these algorithm works! :D"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_l_bfgs_b as fmin\n",
      "\n",
      "# we initialize emm..\n",
      "visibleSize = 8*8\n",
      "hiddenSize = 25\n",
      "sparsityParam = 0.01\n",
      "lamb = 0.0001\n",
      "beta = 3\n",
      "\n",
      "# we initialize the parameters pretraining a DBM\n",
      "\n",
      "r  = 3.1496\n",
      "W1 = uniform(-r,r,(hiddenSize,visibleSize))\n",
      "W2 = uniform(-r,r,(hiddenSize,visibleSize))\n",
      "b1 = np.zeros((hiddenSize,1))\n",
      "b2 = np.zeros((visibleSize,1))\n",
      "\n",
      "# roll the parameters\n",
      "theta = [W1,W2,b1,b2]\n",
      "\n",
      "# use minFunc to minimize the function, nooow! put the mf function.\n",
      "[opttheta, cost] = fmin(sparseAutoencodeCost,theta)\n",
      "\n",
      "\n",
      "\n",
      "     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 5: Visualization\n",
      "====================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}