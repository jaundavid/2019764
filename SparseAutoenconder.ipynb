{
 "metadata": {
  "name": "",
  "signature": "sha256:c55df8ef7fca73685b4123d233181d64290f5d8a111432199d531c4771e406ad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 1: Generate training set\n",
      "============================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.io import loadmat\n",
      "from random import randint\n",
      "\n",
      "def sampleImages():\n",
      "  mat = loadmat('IMAGES.mat')\n",
      "  images = mat['IMAGES']\n",
      "  patchSize = 8\n",
      "  numPatches = 10000\n",
      "  patches = np.zeros((patchSize*patchSize,numPatches))\n",
      "\n",
      "  for i in range(numPatches):\n",
      "    img = randint(0,9)\n",
      "    i,j = randint(0,503),randint(0,503)\n",
      "    patches[:,i] = images[i:i+patchSize,j:j+patchSize,img].reshape(patchSize*patchSize)\n",
      "\n",
      "  return normalizeData(patches)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalizeData(patches):\n",
      "  mean = patches.mean(axis=0)\n",
      "  patches = patches - mean\n",
      "  pstd = 3 * patches.std()\n",
      "  patches = np.maximum(np.minimum(patches, pstd), -pstd) / pstd\n",
      "  patches = (1 + patches) * 0.4 + 0.1\n",
      "  return patches"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 2: Sparse autoencoder objective\n",
      "===================================="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a training set of $m$ examples, we then define the overall cost function to be:\n",
      "\n",
      "$$ \n",
      "\\begin{align}\n",
      "J(W,b)\n",
      "&= \\left[ \\frac{1}{m} \\sum_{i=1}^m J(W,b;x^{(i)},y^{(i)}) \\right]\n",
      "                       + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( W^{(l)}_{ji} \\right)^2\n",
      " \\\\\n",
      "&= \\left[ \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{1}{2} \\left\\| h_{W,b}(x^{(i)}) - y^{(i)} \\right\\|^2 \\right) \\right]\n",
      "                       + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( W^{(l)}_{ji} \\right)^2\n",
      "                       \\end{align}\n",
      "$$\n",
      "\n",
      "And the $J_{sparse}$ as:\n",
      "$$\n",
      "\\begin{align}\n",
      "J_{\\rm sparse}(W,b) = J(W,b) + \\beta \\sum_{j=1}^{s_2} {\\rm KL}(\\rho || \\hat\\rho_j),\n",
      "\\end{align}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Backpropagation propagation algorithm can be written in the following way:\n",
      "\n",
      "\n",
      "1. Perform a feedforward pass, computing the activations for layers $\\textstyle L_2$, $\\textstyle L_3$, up to the output layer $\\textstyle L_{n_l}$, using the equations defining the forward propagation steps\n",
      "2. For the output layer (layer $\\textstyle n_l$), set \n",
      "$$\\begin{align}\n",
      "\\delta^{(n_l)}\n",
      "= - (y - a^{(n_l)}) \\bullet f'(z^{(n_l)})\n",
      "\\end{align}$$\n",
      "4. For $\\textstyle l = n_l-1, n_l-2, n_l-3, \\ldots, 2$ Set\n",
      "$$\n",
      "\\begin{align}\n",
      "\\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\bullet f'(z^{(l)})\n",
      "\\end{align}\n",
      "$$\n",
      "1. Compute the desired partial derivatives: \n",
      "$$\n",
      "\\begin{align}\n",
      "\\nabla_{W^{(l)}} J(W,b;x,y) &= \\delta^{(l+1)} (a^{(l)})^T, \\\\\n",
      "\\nabla_{b^{(l)}} J(W,b;x,y) &= \\delta^{(l+1)}.\n",
      "\\end{align}\n",
      "$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sparseAutoencoderCost(theta, visibleSize, hiddenSize, lamb,\n",
      "                          sparsityParam, beta, data):\n",
      "   #Unroll parameters \n",
      "   W1 = theta[0:hidden_size * visible_size].reshape(hidden_size, visible_size)\n",
      "   W2 = theta[hidden_size * visible_size:2 * hidden_size * visible_size].reshape(visible_size, hidden_size)\n",
      "   b1 = theta[2 * hidden_size * visible_size:2 * hidden_size * visible_size + hidden_size]\n",
      "   b2 = theta[2 * hidden_size * visible_size + hidden_size:]\n",
      "   \n",
      "    #Initialize\n",
      "   cost = 0\n",
      "   W1grad = np.zeros(shape(W1))\n",
      "   W2grad = np.zeros(shape(W2))\n",
      "   b1grad = np.zeros(shape(b1))\n",
      "   b2grad = np.zeros(shape(b2))\n",
      "   \n",
      "   m = len(data)\n",
      "   \n",
      "   #Forward propagation\n",
      "   z2 = W1.dot(data) + b\n",
      "   a2 = sigmoid(z2)\n",
      "   z3 = W2.dot(a2) + b\n",
      "   h = sigmoid(z3) #a3\n",
      "   \n",
      "   sparsityParamHat = (1/m)*sum(a2)\n",
      "   #cost function J(W,b,beta,lambda) \n",
      "   cost = (1/m)*sum((h-data)**2) + (lamb/2)*(sum(W2**2)+sum(W1**2)) + beta*sum(KL(sparsityParamHat,sparsityParam))\n",
      "    \n",
      "   #Backpropagation\n",
      "   delta3 = -(data-h)*sigmoid_prime()\n",
      "   delta2 = np.dot(W2.T,delta3)\n",
      "    \n",
      "   #compute the desired partial derivates\n",
      "   W1grad = delta2*a1.T\n",
      "   W2grad = delta3*a2.T\n",
      "   b1grad = delta2\n",
      "   b2grad = delta3 \n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where the activation function $f(z) = \\frac{1}{{1+e^{-z}}}$, i.e, the sigmoid function, $f'(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i)$ can be calculated in terms of previosly calculated values and we can extend the definition of $f(z)$ and $f'(z)$ in the following way: $f'([z_1, z_2, z_3]) =[f'(z_1),f'(z_2),f'(z_3)]$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sigmoid(x):\n",
      "  return 1 / (1 + np.exp(-x))}\n",
      "    \n",
      "def sigmoiod_prime(x):\n",
      "  return a(1-a)\n",
      "\n",
      "def KL(x,y):\n",
      "  return x*np.log(x/y) + (1-x)*np.log((1- x)/(1-y))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 3: Gradient checking\n",
      "========================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}